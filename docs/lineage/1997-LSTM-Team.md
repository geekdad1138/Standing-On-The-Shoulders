### **1. The Memory Foundation: 1997 — The LSTM Team**

#### **The People**

**Sepp Hochreiter** and **Jürgen Schmidhuber**. Working at the Technical University of Munich, they tackled the "Vanishing Gradient" problem—the math issue that caused early AI to have the "memory of a goldfish."

#### **What They Inherited (The Shoulders)**

* **The Backprop Team (1986):** They inherited the fundamental math of neural networks but realized it broke down over long sequences.
* **John Hopfield (1982):** Inherited the concept of "Recurrent" networks (networks with loops), but improved their stability.

#### **What They Passed On**

* **Long Short-Term Memory (LSTM):** They passed on the "Constant Error Carousel," a way for AI to maintain information over long periods.
* **The Voice Revolution:** This became the engine for **Siri, Google Translate, and Alexa** for nearly two decades.
* **Transformer Foundation:** They passed on the challenge of "Long-range dependencies," which the 2017 Transformer would eventually solve in a new way.
